{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34eb0c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb4278bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Brand Name</th>\n",
       "      <th>Price</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Review Votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>5</td>\n",
       "      <td>I feel so LUCKY to have found this used (phone...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>4</td>\n",
       "      <td>nice phone, nice up grade from my pantach revu...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>5</td>\n",
       "      <td>Very pleased</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>4</td>\n",
       "      <td>It works good but it goes slow sometimes but i...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>4</td>\n",
       "      <td>Great phone to replace my lost phone. The only...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Product Name Brand Name   Price  \\\n",
       "0  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "1  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "2  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "3  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "4  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "\n",
       "   Rating                                            Reviews  Review Votes  \n",
       "0       5  I feel so LUCKY to have found this used (phone...           1.0  \n",
       "1       4  nice phone, nice up grade from my pantach revu...           0.0  \n",
       "2       5                                       Very pleased           0.0  \n",
       "3       4  It works good but it goes slow sometimes but i...           0.0  \n",
       "4       4  Great phone to replace my lost phone. The only...           0.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazonData = pd.read_csv('Amazon_Unlocked_Mobile.csv')\n",
    "amazonData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2808db50",
   "metadata": {},
   "outputs": [],
   "source": [
    "amazonData = amazonData.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba012873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(334335, 6)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazonData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc3021c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "amazonData = amazonData[[\"Reviews\",\"Rating\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "981178be",
   "metadata": {},
   "outputs": [],
   "source": [
    "amazonDataPositiveDf = amazonData[amazonData[\"Rating\"].isin([4,5])]\n",
    "amazonDataNegativeDf = amazonData[amazonData[\"Rating\"].isin([1,2])]\n",
    "amazonDataNeutralDf = amazonData[amazonData[\"Rating\"].isin([3])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa3c7a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive:  (230674, 2)\n",
      "Neutal:  (26058, 2)\n",
      "Negative:  (77603, 2)\n"
     ]
    }
   ],
   "source": [
    "print('Positive: ', amazonDataPositiveDf.shape)\n",
    "print('Neutal: ', amazonDataNeutralDf.shape)\n",
    "print('Negative: ', amazonDataNegativeDf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55e1d164",
   "metadata": {},
   "outputs": [],
   "source": [
    "amazonDataFiltered = pd.concat([amazonDataPositiveDf[:20000], amazonDataNeutralDf[:20000], amazonDataNegativeDf[:20000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27ffab92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.barplot(x=amazonDataFiltered.Rating.value_counts().index, y=amazonDataFiltered.Rating.value_counts().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "275fed1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# amazonDataFiltered[\"Sentiment\"] = \"Positive\"\n",
    "# amazonDataFiltered[\"Sentiment\"][amazonDataFiltered[\"Rating\"].isin([1,2])]= \"Negative\"\n",
    "# amazonDataFiltered[\"Sentiment\"][amazonDataFiltered[\"Rating\"].isin([3])]= \"Neutral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0e6a692",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qy/ltftvghn3vb8lhkfmh8m0tx40000gn/T/ipykernel_32727/2616130612.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  amazonDataFiltered[\"Sentiment\"][amazonDataFiltered[\"Rating\"].isin([1,2])]= 0\n",
      "/var/folders/qy/ltftvghn3vb8lhkfmh8m0tx40000gn/T/ipykernel_32727/2616130612.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  amazonDataFiltered[\"Sentiment\"][amazonDataFiltered[\"Rating\"].isin([3])]= 1\n"
     ]
    }
   ],
   "source": [
    "amazonDataFiltered[\"Sentiment\"] = 2 #positive\n",
    "amazonDataFiltered[\"Sentiment\"][amazonDataFiltered[\"Rating\"].isin([1,2])]= 0 #negative\n",
    "amazonDataFiltered[\"Sentiment\"][amazonDataFiltered[\"Rating\"].isin([3])]= 1 #neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9cc3fb32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazonDataFiltered[20000:20005]\n",
    "amazonDataFiltered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "460de5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import ToktokTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lower_case(text):\n",
    "    return text.lower()\n",
    "def remove_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "def remove_username(text):\n",
    "    return re.sub('@[^\\s]+','',text)\n",
    "def remove_urls(text):\n",
    "    return re.sub(r\"((http\\S+)|(www\\.))\",'',text)\n",
    "def remove_special_characters(text):\n",
    "    pattern = r'[^a-zA-Z\\s]'\n",
    "    text = re.sub(pattern,'',text)\n",
    "    return text\n",
    "def remove_single_char(text):\n",
    "    return re.sub(r'\\b[a-zA-Z]\\b','',text)\n",
    "def remove_multiple(text):\n",
    "    return re.sub(\"(.)\\\\1{2,}\",\"\\\\1\",text)\n",
    "tokenizer = ToktokTokenizer()\n",
    "stopword_list=nltk.corpus.stopwords.words('english')\n",
    "def remove_stopwords(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    filtered_tokens = ' '.join(filtered_tokens)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9add776",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('contractions.json','r') as f:\n",
    "    contractions_dict = json.load(f)\n",
    "contractions = contractions_dict['contractions']\n",
    "def replace_contractions(text):\n",
    "    for word in text.split():\n",
    "        if word.lower()  in contractions:\n",
    "            text = text.replace(word,contractions[word.lower()])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75e02a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('negations.json','r') as f:\n",
    "    neg_dict = json.load(f)\n",
    "negations = neg_dict['negations']\n",
    "\n",
    "#Antonyms\n",
    "#Negation Handler\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "# from nltk.\n",
    "class AntonymReplacer(object):\n",
    "    def replace(self,word):\n",
    "        antonyms = set()\n",
    "        for syn in wordnet.synsets(word):\n",
    "            if syn.pos() in ['a' ,'s']:\n",
    "                for lemma in syn.lemmas():\n",
    "                    for antonym in lemma.antonyms():\n",
    "                        antonyms.add(antonym.name())\n",
    "        if(len(antonyms) == 1):\n",
    "            return antonyms.pop()\n",
    "        else:\n",
    "            if word in negations:\n",
    "                word = word.replace(word,negations[word])\n",
    "                return word\n",
    "        \n",
    "    #Negation Replacer\n",
    "    def negReplacer(self, string):\n",
    "        i=0\n",
    "        finalSent = \"\"\n",
    "        sent = word_tokenize(string)\n",
    "        length_sent = len(sent)\n",
    "        words = []\n",
    "        while i < length_sent:\n",
    "            word = sent[i]\n",
    "            if word == 'not' and i+1 < length_sent:\n",
    "                antonymWord = self.replace(sent[i+1])\n",
    "                if antonymWord:\n",
    "                    words.append(antonymWord)\n",
    "                    finalSent += antonymWord + \" \"\n",
    "                    i += 2\n",
    "                    continue\n",
    "            words.append(word)\n",
    "            finalSent += word + \" \"\n",
    "            i += 1\n",
    "        return finalSent\n",
    "    \n",
    "# replacer = AntonymReplacer()\n",
    "# oppWord = replacer.negReplacer('not recommend')\n",
    "# print(oppWord)\n",
    "    \n",
    "def replace_negation(text):\n",
    "    \n",
    "    replacer = AntonymReplacer()\n",
    "    oppWord = replacer.negReplacer(text)\n",
    "    return oppWord\n",
    "\n",
    "# replace_negation('I am heavy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06761cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "amazonDataFiltered['Reviews'] =amazonDataFiltered['Reviews'].apply(lower_case)\n",
    "amazonDataFiltered['Reviews'] =amazonDataFiltered['Reviews'].apply(remove_multiple)\n",
    "amazonDataFiltered['Reviews'] =amazonDataFiltered['Reviews'].apply(remove_single_char)\n",
    "amazonDataFiltered['Reviews'] =amazonDataFiltered['Reviews'].apply(remove_special_characters)\n",
    "amazonDataFiltered['Reviews'] =amazonDataFiltered['Reviews'].apply(remove_square_brackets)\n",
    "amazonDataFiltered['Reviews'] =amazonDataFiltered['Reviews'].apply(remove_urls)\n",
    "amazonDataFiltered['Reviews'] =amazonDataFiltered['Reviews'].apply(remove_username)\n",
    "amazonDataFiltered['Reviews'] =amazonDataFiltered['Reviews'].apply(replace_contractions)\n",
    "amazonDataFiltered['Reviews'] =amazonDataFiltered['Reviews'].apply(replace_negation)\n",
    "amazonDataFiltered['Reviews'] =amazonDataFiltered['Reviews'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18dbd5a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = amazonDataFiltered[\"Reviews\"]\n",
    "y = amazonDataFiltered[\"Sentiment\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "print(type(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54c92503",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aac</th>\n",
       "      <th>aactivate</th>\n",
       "      <th>aahs</th>\n",
       "      <th>aand</th>\n",
       "      <th>aanother</th>\n",
       "      <th>aaps</th>\n",
       "      <th>aarp</th>\n",
       "      <th>ab</th>\n",
       "      <th>aback</th>\n",
       "      <th>...</th>\n",
       "      <th>zones</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zooming</th>\n",
       "      <th>zooms</th>\n",
       "      <th>zoverall</th>\n",
       "      <th>zsound</th>\n",
       "      <th>ztd</th>\n",
       "      <th>zte</th>\n",
       "      <th>zumbido</th>\n",
       "      <th>zune</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47995</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47996</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47997</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47999</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48000 rows Ã— 28195 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        aa  aac  aactivate  aahs  aand  aanother  aaps  aarp   ab  aback  ...  \\\n",
       "0      0.0  0.0        0.0   0.0   0.0       0.0   0.0   0.0  0.0    0.0  ...   \n",
       "1      0.0  0.0        0.0   0.0   0.0       0.0   0.0   0.0  0.0    0.0  ...   \n",
       "2      0.0  0.0        0.0   0.0   0.0       0.0   0.0   0.0  0.0    0.0  ...   \n",
       "3      0.0  0.0        0.0   0.0   0.0       0.0   0.0   0.0  0.0    0.0  ...   \n",
       "4      0.0  0.0        0.0   0.0   0.0       0.0   0.0   0.0  0.0    0.0  ...   \n",
       "...    ...  ...        ...   ...   ...       ...   ...   ...  ...    ...  ...   \n",
       "47995  0.0  0.0        0.0   0.0   0.0       0.0   0.0   0.0  0.0    0.0  ...   \n",
       "47996  0.0  0.0        0.0   0.0   0.0       0.0   0.0   0.0  0.0    0.0  ...   \n",
       "47997  0.0  0.0        0.0   0.0   0.0       0.0   0.0   0.0  0.0    0.0  ...   \n",
       "47998  0.0  0.0        0.0   0.0   0.0       0.0   0.0   0.0  0.0    0.0  ...   \n",
       "47999  0.0  0.0        0.0   0.0   0.0       0.0   0.0   0.0  0.0    0.0  ...   \n",
       "\n",
       "       zones  zoom  zooming  zooms  zoverall  zsound  ztd  zte  zumbido  zune  \n",
       "0        0.0   0.0      0.0    0.0       0.0     0.0  0.0  0.0      0.0   0.0  \n",
       "1        0.0   0.0      0.0    0.0       0.0     0.0  0.0  0.0      0.0   0.0  \n",
       "2        0.0   0.0      0.0    0.0       0.0     0.0  0.0  0.0      0.0   0.0  \n",
       "3        0.0   0.0      0.0    0.0       0.0     0.0  0.0  0.0      0.0   0.0  \n",
       "4        0.0   0.0      0.0    0.0       0.0     0.0  0.0  0.0      0.0   0.0  \n",
       "...      ...   ...      ...    ...       ...     ...  ...  ...      ...   ...  \n",
       "47995    0.0   0.0      0.0    0.0       0.0     0.0  0.0  0.0      0.0   0.0  \n",
       "47996    0.0   0.0      0.0    0.0       0.0     0.0  0.0  0.0      0.0   0.0  \n",
       "47997    0.0   0.0      0.0    0.0       0.0     0.0  0.0  0.0      0.0   0.0  \n",
       "47998    0.0   0.0      0.0    0.0       0.0     0.0  0.0  0.0      0.0   0.0  \n",
       "47999    0.0   0.0      0.0    0.0       0.0     0.0  0.0  0.0      0.0   0.0  \n",
       "\n",
       "[48000 rows x 28195 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import preprocessing\n",
    "tfidf_vector = TfidfVectorizer()\n",
    "#tfidf_vector = TfidfVectorizer()\n",
    "tfidf_vector.fit(X_train)\n",
    "X_train_data = tfidf_vector.transform(X_train)\n",
    "X_test_data = tfidf_vector.transform(X_test)\n",
    "X_train_data = X_train_data.toarray()\n",
    "X_test_data = X_test_data.toarray()\n",
    "pd.DataFrame(X_train_data, columns=tfidf_vector.get_feature_names())\n",
    "# print(X_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a49f73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# count_vector = CountVectorizer()\n",
    "# count_vector.fit(X_train)\n",
    "# X_train_data = count_vector.transform(X_train)\n",
    "# X_test_data = count_vector.transform(X_test)\n",
    "# X_train_data = X_train_data.toarray()\n",
    "# X_test_data = X_test_data.toarray()\n",
    "# pd.DataFrame(X_test_data[2:6], columns=count_vector.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "13c93e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class naiveBayes:\n",
    "    def predict(self, X_test_data):\n",
    "        y_pred = [self._predict(x, self.log_probabilities, self.class_count) for x in X_test_data]\n",
    "#         print (y_pred)\n",
    "        return y_pred\n",
    "    \n",
    "    def _predict(self, query_point,log_probabilities,classes):\n",
    "        # Calculating posterior probabilities\n",
    "        output = np.matmul(log_probabilities,query_point.T)\n",
    "        # Finding the index using argmax i.e. index of highest probabilty and returing the specified class.\n",
    "        index = np.argmax(output)\n",
    "        #print(index)\n",
    "        return index\n",
    "    \n",
    "    def fit(self, X_train_data, y_train):\n",
    "        n_samples, n_features = X_train_data.shape\n",
    "        self._classes = np.unique(y_train)\n",
    "        y_train = np.array(y_train)\n",
    "        lb = preprocessing.LabelBinarizer()\n",
    "        y_train = lb.fit_transform(y_train)\n",
    "        self.count_matrix = np.matmul(y_train.T,X_train_data)\n",
    "        self.class_count = y_train.sum(axis=0)\n",
    "        n_classes = len(self._classes)\n",
    "#         print(self.class_count)\n",
    "        \n",
    "        def calculate_prior_probs(class_count):\n",
    "            # class count - [8,10]\n",
    "            # probabilities will be 8/18, 10/18\n",
    "            # And we apply log operation on to them.\n",
    "            num = class_count\n",
    "#             print(num)\n",
    "            den = class_count.sum()\n",
    "#             print(den)\n",
    "            prob = np.log(num)-np.log(den)\n",
    "#             print(prob)\n",
    "            return prob\n",
    "\n",
    "        def feature_log_probabilities(count_matrix, prior_prob):\n",
    "            alpha=1\n",
    "            # Adding alpha to the count\n",
    "            smoothed_version = count_matrix+alpha\n",
    "#             print(smoothed_version)\n",
    "            # Calculating the number of words in a given class\n",
    "            den = smoothed_version.sum(axis = 1)\n",
    "            # Reshaping it to 2D column\n",
    "            den = den.reshape(-1,1)\n",
    "            # probability is num/den -- log probability is log(num)- log(den)\n",
    "            #this is conditional probability\n",
    "            log_probabilities = np.log(smoothed_version)-np.log(den)\n",
    "            print('Manually implemented log-probabilities\\n')\n",
    "            print(log_probabilities)\n",
    "            #adding log of conditional probabilities and prior probabilities to find posterior probabilities\n",
    "            def prior_prob_integration(conditional_probabilities):\n",
    "                for idx, x in enumerate(conditional_probabilities):\n",
    "                    for k, y in enumerate(self.prior_prob):\n",
    "                        if (idx == k):\n",
    "                            x += y\n",
    "                return conditional_probabilities\n",
    "            posterior_probabilities = prior_prob_integration(log_probabilities)\n",
    "            return posterior_probabilities\n",
    "        self.prior_prob = calculate_prior_probs(self.class_count)\n",
    "        self.log_probabilities = feature_log_probabilities(self.count_matrix, self.prior_prob)  \n",
    "        print('Manually implemented prior probabilities\\n')\n",
    "        print(self.prior_prob)\n",
    "        print('Manually implemented posterior-probabilities\\n')\n",
    "        print(self.log_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "1356b6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manually implemented log-probabilities\n",
      "\n",
      "[[-11.35014962 -11.35014962 -11.35014962 ... -10.56869904 -11.35014962\n",
      "  -11.35014962]\n",
      " [-11.30144148 -11.15963438  -9.85516006 ... -11.03196231 -11.31684525\n",
      "  -10.48145913]\n",
      " [-10.90393903 -11.17870635 -11.17870635 ... -10.93773846 -11.17870635\n",
      "  -11.17870635]]\n",
      "Manually implemented prior probabilities\n",
      "\n",
      "[-1.09630246 -1.09917495 -1.10036382]\n",
      "Manually implemented posterior-probabilities\n",
      "\n",
      "[[-12.44645208 -12.44645208 -12.44645208 ... -11.6650015  -12.44645208\n",
      "  -12.44645208]\n",
      " [-12.40061643 -12.25880933 -10.954335   ... -12.13113726 -12.4160202\n",
      "  -11.58063407]\n",
      " [-12.00430285 -12.27907017 -12.27907017 ... -12.03810229 -12.27907017\n",
      "  -12.27907017]]\n",
      "Manual predict [1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "MNB = naiveBayes()\n",
    "#MNB = MultinomialNB()\n",
    "MNB.fit(X_train_data, y_train)\n",
    "print('Manual predict', MNB.predict(X_test_data[0:1]))\n",
    "predictions = MNB.predict(X_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "1153888e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn feature log-probabilities\n",
      " [[-11.35014962 -11.35014962 -11.35014962 ... -10.56869904 -11.35014962\n",
      "  -11.35014962]\n",
      " [-11.30144148 -11.15963438  -9.85516006 ... -11.03196231 -11.31684525\n",
      "  -10.48145913]\n",
      " [-10.90393903 -11.17870635 -11.17870635 ... -10.93773846 -11.17870635\n",
      "  -11.17870635]]\n",
      "Sklearn prior probabilities\n",
      " [-1.09630246 -1.09917495 -1.10036382]\n",
      "Sklearn predict [1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "#MNB = naiveBayes()\n",
    "MNB1 = MultinomialNB()\n",
    "MNB1.fit(X_train_data, y_train)\n",
    "predictions1 = MNB1.predict(X_test_data)\n",
    "# predictions = MNB.predict(X_test_data[122:124])\n",
    "# print(predictions)\n",
    "print('Sklearn feature log-probabilities\\n',MNB1.feature_log_prob_)\n",
    "# Comparing predict function\n",
    "print('Sklearn prior probabilities\\n',MNB1.class_log_prior_)\n",
    "print('Sklearn predict',MNB1.predict(X_test_data[0:1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "60bcee5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy of manual model:  0.8294375\n",
      "Test accuracy of manual model: 0.80425\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.83      0.82      3963\n",
      "           1       0.75      0.74      0.75      4009\n",
      "           2       0.86      0.84      0.85      4028\n",
      "\n",
      "    accuracy                           0.80     12000\n",
      "   macro avg       0.80      0.80      0.80     12000\n",
      "weighted avg       0.80      0.80      0.80     12000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "print('Train accuracy of manual model: ',accuracy_score(y_train, MNB.predict(X_train_data)))\n",
    "print('Test accuracy of manual model:', accuracy_score(y_test, predictions))\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "63ccfd4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.8293541666666666\n",
      "Test accuracy: 0.8040833333333334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.83      0.82      3963\n",
      "           1       0.75      0.74      0.75      4009\n",
      "           2       0.86      0.84      0.85      4028\n",
      "\n",
      "    accuracy                           0.80     12000\n",
      "   macro avg       0.80      0.80      0.80     12000\n",
      "weighted avg       0.80      0.80      0.80     12000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "print('Train accuracy: ',accuracy_score(y_train, MNB1.predict(X_train_data)))\n",
    "print('Test accuracy:', accuracy_score(y_test, predictions1))\n",
    "print(classification_report(y_test, predictions1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bfaac787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "165174    aware g gsm phone activate maybe would gophone...\n",
       "300073    well ended getting two one husband mine arrive...\n",
       "21754                                hope iphone six better\n",
       "31104     phones screen broke right away keeps cracking ...\n",
       "34585     thank wonderful phone giving phone gift really...\n",
       "                                ...                        \n",
       "103644    bought phone cheap replacement boyfriend broke...\n",
       "20822      worked great transfered information wham good go\n",
       "32625     phone worked perfectly minimal scratches satis...\n",
       "111014                                               normal\n",
       "13033     disappointed phone spent around hours another ...\n",
       "Name: Reviews, Length: 12000, dtype: object"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2bc76c0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hope iphone six better</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>phones screen broke right away keeps cracking ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>thank wonderful phone giving phone gift really...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>phone perfect send headphones charger genericn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Sentiment\n",
       "0                             hope iphone six better          1\n",
       "1  phones screen broke right away keeps cracking ...          1\n",
       "2  thank wonderful phone giving phone gift really...          2\n",
       "3  phone perfect send headphones charger genericn...          0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_tuples = list(zip(X_test[2:6], predictions))\n",
    "pd.DataFrame(list_of_tuples, columns = ['Text', 'Sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9a89869d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3302,  527,  134],\n",
       "       [ 606, 2978,  425],\n",
       "       [ 206,  453, 3369]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, predictions1)\n",
    "\n",
    "# it shoes that out of 1012 data, 112 data that are acutally positive is predicted as negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6f2df2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11838d6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "97096eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = [\n",
    "#         'Will not repurchase. Do not recommend',\n",
    "#        'I hate this',\n",
    "#        'I love this',\n",
    "#        'This is not worth it',\n",
    "#        'This is not terrible',\n",
    "#        'This product so far has not disappointed',\n",
    "        'Excellent product. Easy to use, large screen makes watching movies and reading easier.',\n",
    "        'I am so happy today',\n",
    "       'Note 10 has great camera quality. I am loving it.',\n",
    "       'I dont know what is wrong with this phone. I have been trying to type but its not working.',\n",
    "       'Dell laptop battery dead',\n",
    "        'Good, but unhappy that screen size is small, less than I expected',\n",
    "       'The product has not disappointed',\n",
    "       'It is a expensive phone',\n",
    "       'I hate apple',\n",
    "        'less expensive than last year with so many more features and bigger screen!',\n",
    "        'I do not enjoy working under tight deadline',\n",
    "        'worst phone in the history',\n",
    "        'phone is not good but has nice screen'\n",
    "]\n",
    "# print(text1)\n",
    "text_df1 = pd.DataFrame(text1,columns=['text'])\n",
    "# print(text_df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ad6ad82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df1['text'] =text_df1['text'].apply(lower_case)\n",
    "text_df1['text'] =text_df1['text'].apply(remove_multiple)\n",
    "text_df1['text'] =text_df1['text'].apply(remove_single_char)\n",
    "text_df1['text'] =text_df1['text'].apply(remove_special_characters)\n",
    "# text_df1['text'] =text_df1['text'].apply(remove_stopwords)\n",
    "text_df1['text'] =text_df1['text'].apply(remove_square_brackets)\n",
    "text_df1['text'] =text_df1['text'].apply(remove_urls)\n",
    "text_df1['text'] =text_df1['text'].apply(remove_username)\n",
    "text_df1['text'] =text_df1['text'].apply(replace_contractions)\n",
    "text_df1['text'] =text_df1['text'].apply(replace_negation)\n",
    "# text_df1['text'] =text_df1['text'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bac688e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=tfidf_vector.transform(text_df1['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e09446b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sentences = [\n",
    "    'I like this phone', \n",
    "    'The screen is small in this phone', \n",
    "    'I prefer big screen over small in a phone'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7c8a65bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>big</th>\n",
       "      <th>in</th>\n",
       "      <th>is</th>\n",
       "      <th>like</th>\n",
       "      <th>over</th>\n",
       "      <th>phone</th>\n",
       "      <th>prefer</th>\n",
       "      <th>screen</th>\n",
       "      <th>small</th>\n",
       "      <th>the</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.720333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.425441</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.547832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.352215</td>\n",
       "      <td>0.463121</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.273526</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.352215</td>\n",
       "      <td>0.352215</td>\n",
       "      <td>0.463121</td>\n",
       "      <td>0.352215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.443503</td>\n",
       "      <td>0.337295</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.443503</td>\n",
       "      <td>0.261940</td>\n",
       "      <td>0.443503</td>\n",
       "      <td>0.337295</td>\n",
       "      <td>0.337295</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        big        in        is      like      over     phone    prefer  \\\n",
       "0  0.000000  0.000000  0.000000  0.720333  0.000000  0.425441  0.000000   \n",
       "1  0.000000  0.352215  0.463121  0.000000  0.000000  0.273526  0.000000   \n",
       "2  0.443503  0.337295  0.000000  0.000000  0.443503  0.261940  0.443503   \n",
       "\n",
       "     screen     small       the      this  \n",
       "0  0.000000  0.000000  0.000000  0.547832  \n",
       "1  0.352215  0.352215  0.463121  0.352215  \n",
       "2  0.337295  0.337295  0.000000  0.000000  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "vectorizer = tfidf.fit_transform(sample_sentences)\n",
    "pd.DataFrame(vectorizer.toarray(), columns=tfidf.get_feature_names())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
