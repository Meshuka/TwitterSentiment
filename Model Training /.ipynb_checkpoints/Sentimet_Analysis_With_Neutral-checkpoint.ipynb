{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "34eb0c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "eb4278bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Brand Name</th>\n",
       "      <th>Price</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Review Votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>5</td>\n",
       "      <td>I feel so LUCKY to have found this used (phone...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>4</td>\n",
       "      <td>nice phone, nice up grade from my pantach revu...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>5</td>\n",
       "      <td>Very pleased</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>4</td>\n",
       "      <td>It works good but it goes slow sometimes but i...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>4</td>\n",
       "      <td>Great phone to replace my lost phone. The only...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Product Name Brand Name   Price  \\\n",
       "0  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "1  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "2  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "3  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "4  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "\n",
       "   Rating                                            Reviews  Review Votes  \n",
       "0       5  I feel so LUCKY to have found this used (phone...           1.0  \n",
       "1       4  nice phone, nice up grade from my pantach revu...           0.0  \n",
       "2       5                                       Very pleased           0.0  \n",
       "3       4  It works good but it goes slow sometimes but i...           0.0  \n",
       "4       4  Great phone to replace my lost phone. The only...           0.0  "
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazonData = pd.read_csv('Amazon_Unlocked_Mobile.csv')\n",
    "amazonData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "2808db50",
   "metadata": {},
   "outputs": [],
   "source": [
    "amazonData = amazonData.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "ba012873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(334335, 6)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazonData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "fc3021c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "amazonData = amazonData[[\"Reviews\",\"Rating\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "981178be",
   "metadata": {},
   "outputs": [],
   "source": [
    "amazonDataPositiveDf = amazonData[amazonData[\"Rating\"].isin([4,5])]\n",
    "amazonDataNegativeDf = amazonData[amazonData[\"Rating\"].isin([1,2])]\n",
    "amazonDataNeutralDf = amazonData[amazonData[\"Rating\"].isin([3])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "aa3c7a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive:  (230674, 2)\n",
      "Neutal:  (26058, 2)\n",
      "Negative:  (77603, 2)\n"
     ]
    }
   ],
   "source": [
    "print('Positive: ', amazonDataPositiveDf.shape)\n",
    "print('Neutal: ', amazonDataNeutralDf.shape)\n",
    "print('Negative: ', amazonDataNegativeDf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "55e1d164",
   "metadata": {},
   "outputs": [],
   "source": [
    "amazonDataFiltered = pd.concat([amazonDataPositiveDf[:20000], amazonDataNeutralDf[:20000], amazonDataNegativeDf[:20000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "27ffab92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD4CAYAAADsKpHdAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAUuUlEQVR4nO3df4xd9Znf8fendpbSJGYJTJDrMTUbnKjgtk49cpFQsmm9XbxpGpMKWiMVXNXVJAiqRInawlZq0pUsLd0mVLSLV86CwGmWHwthcduwDYU0KCsDOyYOxhCaSWDDxBZ2AiWOstDaefrH/U65jK9nxneGuUP8fklX99znnO+Z595/Pvd8z7lzUlVIkvQXBt2AJGlxMBAkSYCBIElqDARJEmAgSJKapYNuoF9nn312rVq1atBtSNJbyp49e35UVUO91r1lA2HVqlWMjY0Nug1JektJ8mcnWueUkSQJMBAkSY2BIEkCDARJUmMgSJIAA0GS1MwYCElWJvl6kmeS7E/yyVZ/V5IHk3y3PZ/ZNeb6JONJnk1ySVd9XZJ9bd1NSdLqpyW5q9UfS7LqTXivkqRpzOYI4Sjwmar6q8BFwDVJLgCuAx6qqtXAQ+01bd1m4EJgI3BzkiVtX9uBUWB1e2xs9a3Ay1V1PnAjcMM8vDdJ0kmYMRCq6mBVPdGWjwDPACuATcDtbbPbgUvb8ibgzqp6raqeA8aB9UmWA8uqand1bsKwc8qYyX3dA2yYPHqQJC2Mk/qlcpvKeT/wGHBOVR2ETmgkeXfbbAXwaNewiVb7v215an1yzAttX0eTvAKcBfxoyt8fpXOEwbnnnnsyrUtc/B8vHnQL8+5P/vmfDLoF/QKZ9UnlJO8A7gU+VVU/mW7THrWapj7dmDcWqnZU1UhVjQwN9fxXHJKkPs0qEJK8jU4YfLmqvtLKL7ZpINrzoVafAFZ2DR8GDrT6cI/6G8YkWQqcAbx0sm9GktS/2VxlFOAW4Jmq+kLXql3Alra8Bbi/q765XTl0Hp2Tx4+36aUjSS5q+7xqypjJfV0GPFze7FmSFtRsziFcDFwJ7Euyt9V+E/ht4O4kW4EfAJcDVNX+JHcDT9O5QumaqjrWxl0N3AacDjzQHtAJnC8lGadzZLB5bm9LknSyZgyEqvomvef4ATacYMw2YFuP+hiwpkf9VVqgSJIGw18qS5IAA0GS1BgIkiTAQJAkNQaCJAkwECRJjYEgSQIMBElSYyBIkgADQZLUGAiSJMBAkCQ1BoIkCTAQJEmNgSBJAgwESVIzm1to3prkUJKnump3JdnbHs9P3kktyaokf9617ve6xqxLsi/JeJKb2m00abfavKvVH0uyav7fpiRpJrM5QrgN2NhdqKp/VFVrq2otcC/wla7V35tcV1Wf6KpvB0bp3GN5ddc+twIvV9X5wI3ADf28EUnS3MwYCFX1CJ37HB+nfcv/h8Ad0+0jyXJgWVXtrqoCdgKXttWbgNvb8j3AhsmjB0nSwpnrOYQPAC9W1Xe7aucl+VaSbyT5QKutACa6tplotcl1LwBU1VHgFeCsOfYlSTpJS+c4/greeHRwEDi3qn6cZB3wR0kuBHp946/2PN26N0gySmfaiXPPPbfvpiVJx+v7CCHJUuAfAHdN1qrqtar6cVveA3wPeC+dI4LhruHDwIG2PAGs7NrnGZxgiqqqdlTVSFWNDA0N9du6JKmHuUwZ/Rrwnar6/1NBSYaSLGnLv0Ln5PH3q+ogcCTJRe38wFXA/W3YLmBLW74MeLidZ5AkLaDZXHZ6B7AbeF+SiSRb26rNHH8y+YPAk0m+TecE8SeqavLb/tXA7wPjdI4cHmj1W4CzkowDnwaum8P7kST1acZzCFV1xQnq/6RH7V46l6H22n4MWNOj/ipw+Ux9SJLeXP5SWZIEGAiSpMZAkCQBBoIkqTEQJEmAgSBJagwESRIw9/9lJElvaf/pM/9l0C3Mu2s///f7GucRgiQJMBAkSY2BIEkCDARJUvMLeVJ53b/YOegW5t2e37lq0C1I+gXnEYIkCTAQJEmNgSBJAgwESVIzm1to3prkUJKnumqfS/LDJHvb48Nd665PMp7k2SSXdNXXJdnX1t3U7q1MktOS3NXqjyVZNc/vUZI0C7M5QrgN2NijfmNVrW2PrwIkuYDOvZYvbGNuTrKkbb8dGAVWt8fkPrcCL1fV+cCNwA19vhdJ0hzMGAhV9Qjw0iz3twm4s6peq6rngHFgfZLlwLKq2l1VBewELu0ac3tbvgfYMHn0IElaOHM5h3BtkifblNKZrbYCeKFrm4lWW9GWp9bfMKaqjgKvAGf1+oNJRpOMJRk7fPjwHFqXJE3VbyBsB94DrAUOAp9v9V7f7Gua+nRjji9W7aiqkaoaGRoaOqmGJUnT6ysQqurFqjpWVT8Hvgisb6smgJVdmw4DB1p9uEf9DWOSLAXOYPZTVJKkedJXILRzApM+BkxegbQL2NyuHDqPzsnjx6vqIHAkyUXt/MBVwP1dY7a05cuAh9t5BknSAprxfxkluQP4EHB2kgngs8CHkqylM7XzPPBxgKran+Ru4GngKHBNVR1ru7qazhVLpwMPtAfALcCXkozTOTLYPA/vS5J0kmYMhKq6okf5lmm23wZs61EfA9b0qL8KXD5TH5KkN5e/VJYkAQaCJKkxECRJgIEgSWoMBEkSYCBIkhoDQZIEGAiSpMZAkCQBBoIkqTEQJEmAgSBJagwESRJgIEiSGgNBkgQYCJKkxkCQJAGzCIQktyY5lOSprtrvJPlOkieT3Jfkl1t9VZI/T7K3PX6va8y6JPuSjCe5qd1bmXb/5bta/bEkq+b/bUqSZjKbI4TbgI1Tag8Ca6rqrwP/C7i+a933qmpte3yiq74dGAVWt8fkPrcCL1fV+cCNwA0n/S4kSXM2YyBU1SPAS1NqX6uqo+3lo8DwdPtIshxYVlW7q6qAncClbfUm4Pa2fA+wYfLoQZK0cObjHMI/BR7oen1ekm8l+UaSD7TaCmCia5uJVptc9wJAC5lXgLN6/aEko0nGkowdPnx4HlqXJE2aUyAk+dfAUeDLrXQQOLeq3g98GviDJMuAXt/4a3I306x7Y7FqR1WNVNXI0NDQXFqXJE2xtN+BSbYAHwE2tGkgquo14LW2vCfJ94D30jki6J5WGgYOtOUJYCUwkWQpcAZTpqgkSW++vo4QkmwE/hXw0ar6WVd9KMmStvwrdE4ef7+qDgJHklzUzg9cBdzfhu0CtrTly4CHJwNGkrRwZjxCSHIH8CHg7CQTwGfpXFV0GvBgO//7aLui6IPAbyU5ChwDPlFVk9/2r6ZzxdLpdM45TJ53uAX4UpJxOkcGm+flnUmSTsqMgVBVV/Qo33KCbe8F7j3BujFgTY/6q8DlM/UhSXpz+UtlSRJgIEiSGgNBkgQYCJKkxkCQJAEGgiSpMRAkSYCBIElqDARJEmAgSJIaA0GSBBgIkqTGQJAkAQaCJKkxECRJgIEgSWoMBEkSMItASHJrkkNJnuqqvSvJg0m+257P7Fp3fZLxJM8muaSrvi7JvrbupnZvZZKcluSuVn8syap5fo+SpFmYzRHCbcDGKbXrgIeqajXwUHtNkgvo3BP5wjbm5iRL2pjtwCiwuj0m97kVeLmqzgduBG7o981Ikvo3YyBU1SPAS1PKm4Db2/LtwKVd9Tur6rWqeg4YB9YnWQ4sq6rdVVXAziljJvd1D7Bh8uhBkrRw+j2HcE5VHQRoz+9u9RXAC13bTbTairY8tf6GMVV1FHgFOKvXH00ymmQsydjhw4f7bF2S1Mt8n1Tu9c2+pqlPN+b4YtWOqhqpqpGhoaE+W5Qk9dJvILzYpoFoz4dafQJY2bXdMHCg1Yd71N8wJslS4AyOn6KSJL3J+g2EXcCWtrwFuL+rvrldOXQenZPHj7dppSNJLmrnB66aMmZyX5cBD7fzDJKkBbR0pg2S3AF8CDg7yQTwWeC3gbuTbAV+AFwOUFX7k9wNPA0cBa6pqmNtV1fTuWLpdOCB9gC4BfhSknE6Rwab5+WdSZJOyoyBUFVXnGDVhhNsvw3Y1qM+BqzpUX+VFiiSpMHxl8qSJMBAkCQ1BoIkCTAQJEmNgSBJAgwESVJjIEiSAANBktQYCJIkwECQJDUGgiQJMBAkSY2BIEkCDARJUmMgSJIAA0GS1BgIkiRgDoGQ5H1J9nY9fpLkU0k+l+SHXfUPd425Psl4kmeTXNJVX5dkX1t3U7vvsiRpAfUdCFX1bFWtraq1wDrgZ8B9bfWNk+uq6qsASS6gc7/kC4GNwM1JlrTttwOjwOr22NhvX5Kk/szXlNEG4HtV9WfTbLMJuLOqXquq54BxYH2S5cCyqtpdVQXsBC6dp74kSbM0X4GwGbij6/W1SZ5McmuSM1ttBfBC1zYTrbaiLU+tHyfJaJKxJGOHDx+ep9YlSTAPgZDkl4CPAn/YStuB9wBrgYPA5yc37TG8pqkfX6zaUVUjVTUyNDQ0l7YlSVPMxxHCbwBPVNWLAFX1YlUdq6qfA18E1rftJoCVXeOGgQOtPtyjLklaQPMRCFfQNV3UzglM+hjwVFveBWxOclqS8+icPH68qg4CR5Jc1K4uugq4fx76kiSdhKVzGZzkLwF/F/h4V/nfJVlLZ9rn+cl1VbU/yd3A08BR4JqqOtbGXA3cBpwOPNAekqQFNKdAqKqfAWdNqV05zfbbgG096mPAmrn0IkmaG3+pLEkCDARJUmMgSJIAA0GS1BgIkiTAQJAkNQaCJAkwECRJjYEgSQIMBElSYyBIkgADQZLUGAiSJMBAkCQ1BoIkCTAQJEmNgSBJAuYYCEmeT7Ivyd4kY632riQPJvluez6za/vrk4wneTbJJV31dW0/40luavdWliQtoPk4QvjbVbW2qkba6+uAh6pqNfBQe02SC4DNwIXARuDmJEvamO3AKLC6PTbOQ1+SpJPwZkwZbQJub8u3A5d21e+sqteq6jlgHFifZDmwrKp2V1UBO7vGSJIWyFwDoYCvJdmTZLTVzqmqgwDt+d2tvgJ4oWvsRKutaMtT68dJMppkLMnY4cOH59i6JKnb0jmOv7iqDiR5N/Bgku9Ms22v8wI1Tf34YtUOYAfAyMhIz20kSf2Z0xFCVR1oz4eA+4D1wIttGoj2fKhtPgGs7Bo+DBxo9eEedUnSAuo7EJK8Pck7J5eBXweeAnYBW9pmW4D72/IuYHOS05KcR+fk8eNtWulIkova1UVXdY2RJC2QuUwZnQPc164QXQr8QVX9cZI/Be5OshX4AXA5QFXtT3I38DRwFLimqo61fV0N3AacDjzQHpKkBdR3IFTV94G/0aP+Y2DDCcZsA7b1qI8Ba/rtRZI0d/5SWZIEGAiSpGaul51qkfvBb/21Qbcw7879N/sG3YL0C8kjBEkSYCBIkhoDQZIEGAiSpMaTytIp6Bsf/NVBtzDvfvWRbwy6hbc8jxAkSYCBIElqDARJEmAgSJIaA0GSBBgIkqTGQJAkAQaCJKkxECRJwNzuqbwyydeTPJNkf5JPtvrnkvwwyd72+HDXmOuTjCd5NsklXfV1Sfa1dTe1eytLkhbQXP51xVHgM1X1RJJ3AnuSPNjW3VhV/7574yQXAJuBC4G/DPyPJO9t91XeDowCjwJfBTbifZUlaUH1fYRQVQer6om2fAR4BlgxzZBNwJ1V9VpVPQeMA+uTLAeWVdXuqipgJ3Bpv31JkvozL+cQkqwC3g881krXJnkyya1Jzmy1FcALXcMmWm1FW55a7/V3RpOMJRk7fPjwfLQuSWrmHAhJ3gHcC3yqqn5CZ/rnPcBa4CDw+clNewyvaerHF6t2VNVIVY0MDQ3NtXVJUpc5BUKSt9EJgy9X1VcAqurFqjpWVT8Hvgisb5tPACu7hg8DB1p9uEddkrSA5nKVUYBbgGeq6gtd9eVdm30MeKot7wI2JzktyXnAauDxqjoIHElyUdvnVcD9/fYlSerPXK4yuhi4EtiXZG+r/SZwRZK1dKZ9ngc+DlBV+5PcDTxN5wqla9oVRgBXA7cBp9O5usgrjCRpgfUdCFX1TXrP/391mjHbgG096mPAmn57kSTNnb9UliQBBoIkqTEQJEmAgSBJagwESRJgIEiSGgNBkgQYCJKkxkCQJAEGgiSpMRAkSYCBIElqDARJEmAgSJIaA0GSBBgIkqTGQJAkAYsoEJJsTPJskvEk1w26H0k61SyKQEiyBPhd4DeAC+jcl/mCwXYlSaeWRREIwHpgvKq+X1X/B7gT2DTgniTplJKqGnQPJLkM2FhV/6y9vhL4W1V17ZTtRoHR9vJ9wLML2mhvZwM/GnQTi4SfRYefw+v8LF63WD6Lv1JVQ71WLF3oTk4gPWrHJVVV7QB2vPntzF6SsaoaGXQfi4GfRYefw+v8LF73VvgsFsuU0QSwsuv1MHBgQL1I0ilpsQTCnwKrk5yX5JeAzcCuAfckSaeURTFlVFVHk1wL/HdgCXBrVe0fcFuztaimsAbMz6LDz+F1fhavW/SfxaI4qSxJGrzFMmUkSRowA0GSBBgIfUlya5JDSZ4adC+DlmRlkq8neSbJ/iSfHHRPg5LkLyZ5PMm322fxbwfd06AlWZLkW0n+66B7GaQkzyfZl2RvkrFB93MinkPoQ5IPAj8FdlbVmkH3M0hJlgPLq+qJJO8E9gCXVtXTA25twSUJ8Paq+mmStwHfBD5ZVY8OuLWBSfJpYARYVlUfGXQ/g5LkeWCkqhbDD9NOyCOEPlTVI8BLg+5jMaiqg1X1RFs+AjwDrBhsV4NRHT9tL9/WHqfsN64kw8DfA35/0L1odgwEzZskq4D3A48NuJWBaVMke4FDwINVdcp+FsB/AP4l8PMB97EYFPC1JHvav+BZlAwEzYsk7wDuBT5VVT8ZdD+DUlXHqmotnV/br09ySk4pJvkIcKiq9gy6l0Xi4qr6m3T+o/M1bdp50TEQNGdtvvxe4MtV9ZVB97MYVNX/Bv4nsHGwnQzMxcBH29z5ncDfSfKfB9vS4FTVgfZ8CLiPzn94XnQMBM1JO5F6C/BMVX1h0P0MUpKhJL/clk8Hfg34zkCbGpCqur6qhqtqFZ1/RfNwVf3jAbc1EEne3i64IMnbgV8HFuUVigZCH5LcAewG3pdkIsnWQfc0QBcDV9L5Bri3PT486KYGZDnw9SRP0vn/XA9W1Sl9uaUAOAf4ZpJvA48D/62q/njAPfXkZaeSJMAjBElSYyBIkgADQZLUGAiSJMBAkCQ1BoIkCTAQJEnN/wOqFzInzzWd8gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.barplot(x=amazonDataFiltered.Rating.value_counts().index, y=amazonDataFiltered.Rating.value_counts().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "275fed1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# amazonDataFiltered[\"Sentiment\"] = \"Positive\"\n",
    "# amazonDataFiltered[\"Sentiment\"][amazonDataFiltered[\"Rating\"].isin([1,2])]= \"Negative\"\n",
    "# amazonDataFiltered[\"Sentiment\"][amazonDataFiltered[\"Rating\"].isin([3])]= \"Neutral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "a0e6a692",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qy/ltftvghn3vb8lhkfmh8m0tx40000gn/T/ipykernel_2668/2616130612.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  amazonDataFiltered[\"Sentiment\"][amazonDataFiltered[\"Rating\"].isin([1,2])]= 0\n",
      "/var/folders/qy/ltftvghn3vb8lhkfmh8m0tx40000gn/T/ipykernel_2668/2616130612.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  amazonDataFiltered[\"Sentiment\"][amazonDataFiltered[\"Rating\"].isin([3])]= 1\n"
     ]
    }
   ],
   "source": [
    "amazonDataFiltered[\"Sentiment\"] = 2\n",
    "amazonDataFiltered[\"Sentiment\"][amazonDataFiltered[\"Rating\"].isin([1,2])]= 0\n",
    "amazonDataFiltered[\"Sentiment\"][amazonDataFiltered[\"Rating\"].isin([3])]= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "9cc3fb32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 3)"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazonDataFiltered[20000:20005]\n",
    "amazonDataFiltered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "460de5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import ToktokTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lower_case(text):\n",
    "    return text.lower()\n",
    "def remove_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "def remove_username(text):\n",
    "    return re.sub('@[^\\s]+','',text)\n",
    "def remove_urls(text):\n",
    "    return re.sub(r\"((http\\S+)|(www\\.))\",'',text)\n",
    "def remove_special_characters(text):\n",
    "    pattern = r'[^a-zA-Z\\s]'\n",
    "    text = re.sub(pattern,'',text)\n",
    "    return text\n",
    "def remove_single_char(text):\n",
    "    return re.sub(r'\\b[a-zA-Z]\\b','',text)\n",
    "def remove_multiple(text):\n",
    "    return re.sub(\"(.)\\\\1{2,}\",\"\\\\1\",text)\n",
    "tokenizer = ToktokTokenizer()\n",
    "stopword_list=nltk.corpus.stopwords.words('english')\n",
    "def remove_stopwords(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    filtered_tokens = ' '.join(filtered_tokens)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "e9add776",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('contractions.json','r') as f:\n",
    "    contractions_dict = json.load(f)\n",
    "contractions = contractions_dict['contractions']\n",
    "def replace_contractions(text):\n",
    "    for word in text.split():\n",
    "        if word.lower()  in contractions:\n",
    "            text = text.replace(word,contractions[word.lower()])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "75e02a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('negations.json','r') as f:\n",
    "    neg_dict = json.load(f)\n",
    "negations = neg_dict['negations']\n",
    "\n",
    "#Antonyms\n",
    "#Negation Handler\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "# from nltk.\n",
    "class AntonymReplacer(object):\n",
    "    def replace(self,word):\n",
    "        antonyms = set()\n",
    "        for syn in wordnet.synsets(word):\n",
    "            if syn.pos() in ['a' ,'s']:\n",
    "                for lemma in syn.lemmas():\n",
    "                    for antonym in lemma.antonyms():\n",
    "                        antonyms.add(antonym.name())\n",
    "        if(len(antonyms) == 1):\n",
    "            return antonyms.pop()\n",
    "        else:\n",
    "            if word in negations:\n",
    "                word = word.replace(word,negations[word])\n",
    "                return word\n",
    "        \n",
    "    #Negation Replacer\n",
    "    def negReplacer(self, string):\n",
    "        i=0\n",
    "        finalSent = \"\"\n",
    "        sent = word_tokenize(string)\n",
    "        length_sent = len(sent)\n",
    "        words = []\n",
    "        while i < length_sent:\n",
    "            word = sent[i]\n",
    "            if word == 'not' and i+1 < length_sent:\n",
    "                antonymWord = self.replace(sent[i+1])\n",
    "                if antonymWord:\n",
    "                    words.append(antonymWord)\n",
    "                    finalSent += antonymWord + \" \"\n",
    "                    i += 2\n",
    "                    continue\n",
    "            words.append(word)\n",
    "            finalSent += word + \" \"\n",
    "            i += 1\n",
    "        return finalSent\n",
    "    \n",
    "# replacer = AntonymReplacer()\n",
    "# oppWord = replacer.negReplacer('not recommend')\n",
    "# print(oppWord)\n",
    "    \n",
    "def replace_negation(text):\n",
    "    \n",
    "    replacer = AntonymReplacer()\n",
    "    oppWord = replacer.negReplacer(text)\n",
    "    return oppWord\n",
    "\n",
    "# replace_negation('I am heavy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "06761cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "amazonDataFiltered['Reviews'] =amazonDataFiltered['Reviews'].apply(lower_case)\n",
    "amazonDataFiltered['Reviews'] =amazonDataFiltered['Reviews'].apply(remove_multiple)\n",
    "amazonDataFiltered['Reviews'] =amazonDataFiltered['Reviews'].apply(remove_single_char)\n",
    "amazonDataFiltered['Reviews'] =amazonDataFiltered['Reviews'].apply(remove_special_characters)\n",
    "amazonDataFiltered['Reviews'] =amazonDataFiltered['Reviews'].apply(remove_square_brackets)\n",
    "amazonDataFiltered['Reviews'] =amazonDataFiltered['Reviews'].apply(remove_urls)\n",
    "amazonDataFiltered['Reviews'] =amazonDataFiltered['Reviews'].apply(remove_username)\n",
    "amazonDataFiltered['Reviews'] =amazonDataFiltered['Reviews'].apply(replace_contractions)\n",
    "amazonDataFiltered['Reviews'] =amazonDataFiltered['Reviews'].apply(replace_negation)\n",
    "amazonDataFiltered['Reviews'] =amazonDataFiltered['Reviews'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "18dbd5a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = amazonDataFiltered[\"Reviews\"]\n",
    "y = amazonDataFiltered[\"Sentiment\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "print(type(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "54c92503",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aac</th>\n",
       "      <th>aactivate</th>\n",
       "      <th>aahs</th>\n",
       "      <th>aand</th>\n",
       "      <th>aanother</th>\n",
       "      <th>aaps</th>\n",
       "      <th>aarp</th>\n",
       "      <th>ab</th>\n",
       "      <th>aback</th>\n",
       "      <th>...</th>\n",
       "      <th>zones</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zoomed</th>\n",
       "      <th>zooming</th>\n",
       "      <th>zooms</th>\n",
       "      <th>zs</th>\n",
       "      <th>ztd</th>\n",
       "      <th>zte</th>\n",
       "      <th>zumbido</th>\n",
       "      <th>zune</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47995</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47996</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47997</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47999</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48000 rows × 28096 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        aa  aac  aactivate  aahs  aand  aanother  aaps  aarp   ab  aback  ...  \\\n",
       "0      0.0  0.0        0.0   0.0   0.0       0.0   0.0   0.0  0.0    0.0  ...   \n",
       "1      0.0  0.0        0.0   0.0   0.0       0.0   0.0   0.0  0.0    0.0  ...   \n",
       "2      0.0  0.0        0.0   0.0   0.0       0.0   0.0   0.0  0.0    0.0  ...   \n",
       "3      0.0  0.0        0.0   0.0   0.0       0.0   0.0   0.0  0.0    0.0  ...   \n",
       "4      0.0  0.0        0.0   0.0   0.0       0.0   0.0   0.0  0.0    0.0  ...   \n",
       "...    ...  ...        ...   ...   ...       ...   ...   ...  ...    ...  ...   \n",
       "47995  0.0  0.0        0.0   0.0   0.0       0.0   0.0   0.0  0.0    0.0  ...   \n",
       "47996  0.0  0.0        0.0   0.0   0.0       0.0   0.0   0.0  0.0    0.0  ...   \n",
       "47997  0.0  0.0        0.0   0.0   0.0       0.0   0.0   0.0  0.0    0.0  ...   \n",
       "47998  0.0  0.0        0.0   0.0   0.0       0.0   0.0   0.0  0.0    0.0  ...   \n",
       "47999  0.0  0.0        0.0   0.0   0.0       0.0   0.0   0.0  0.0    0.0  ...   \n",
       "\n",
       "       zones  zoom  zoomed  zooming  zooms   zs  ztd  zte  zumbido  zune  \n",
       "0        0.0   0.0     0.0      0.0    0.0  0.0  0.0  0.0      0.0   0.0  \n",
       "1        0.0   0.0     0.0      0.0    0.0  0.0  0.0  0.0      0.0   0.0  \n",
       "2        0.0   0.0     0.0      0.0    0.0  0.0  0.0  0.0      0.0   0.0  \n",
       "3        0.0   0.0     0.0      0.0    0.0  0.0  0.0  0.0      0.0   0.0  \n",
       "4        0.0   0.0     0.0      0.0    0.0  0.0  0.0  0.0      0.0   0.0  \n",
       "...      ...   ...     ...      ...    ...  ...  ...  ...      ...   ...  \n",
       "47995    0.0   0.0     0.0      0.0    0.0  0.0  0.0  0.0      0.0   0.0  \n",
       "47996    0.0   0.0     0.0      0.0    0.0  0.0  0.0  0.0      0.0   0.0  \n",
       "47997    0.0   0.0     0.0      0.0    0.0  0.0  0.0  0.0      0.0   0.0  \n",
       "47998    0.0   0.0     0.0      0.0    0.0  0.0  0.0  0.0      0.0   0.0  \n",
       "47999    0.0   0.0     0.0      0.0    0.0  0.0  0.0  0.0      0.0   0.0  \n",
       "\n",
       "[48000 rows x 28096 columns]"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import preprocessing\n",
    "tfidf_vector = TfidfVectorizer()\n",
    "#tfidf_vector = TfidfVectorizer()\n",
    "tfidf_vector.fit(X_train)\n",
    "X_train_data = tfidf_vector.transform(X_train)\n",
    "X_test_data = tfidf_vector.transform(X_test)\n",
    "X_train_data = X_train_data.toarray()\n",
    "X_test_data = X_test_data.toarray()\n",
    "pd.DataFrame(X_train_data, columns=tfidf_vector.get_feature_names())\n",
    "# print(X_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "0a49f73b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aac</th>\n",
       "      <th>aactivate</th>\n",
       "      <th>aahs</th>\n",
       "      <th>aand</th>\n",
       "      <th>aanother</th>\n",
       "      <th>aaps</th>\n",
       "      <th>aarp</th>\n",
       "      <th>ab</th>\n",
       "      <th>aback</th>\n",
       "      <th>...</th>\n",
       "      <th>zones</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zoomed</th>\n",
       "      <th>zooming</th>\n",
       "      <th>zooms</th>\n",
       "      <th>zs</th>\n",
       "      <th>ztd</th>\n",
       "      <th>zte</th>\n",
       "      <th>zumbido</th>\n",
       "      <th>zune</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 28096 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aa  aac  aactivate  aahs  aand  aanother  aaps  aarp  ab  aback  ...  \\\n",
       "0   0    0          0     0     0         0     0     0   0      0  ...   \n",
       "1   0    0          0     0     0         0     0     0   0      0  ...   \n",
       "2   0    0          0     0     0         0     0     0   0      0  ...   \n",
       "3   0    0          0     0     0         0     0     0   0      0  ...   \n",
       "\n",
       "   zones  zoom  zoomed  zooming  zooms  zs  ztd  zte  zumbido  zune  \n",
       "0      0     0       0        0      0   0    0    0        0     0  \n",
       "1      0     0       0        0      0   0    0    0        0     0  \n",
       "2      0     0       0        0      0   0    0    0        0     0  \n",
       "3      0     0       0        0      0   0    0    0        0     0  \n",
       "\n",
       "[4 rows x 28096 columns]"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# count_vector = CountVectorizer()\n",
    "# count_vector.fit(X_train)\n",
    "# X_train_data = count_vector.transform(X_train)\n",
    "# X_test_data = count_vector.transform(X_test)\n",
    "# X_train_data = X_train_data.toarray()\n",
    "# X_test_data = X_test_data.toarray()\n",
    "# pd.DataFrame(X_test_data[2:6], columns=count_vector.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "13c93e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class naiveBayes:\n",
    "    \n",
    "    def fit(self, X_train_data, y_train):\n",
    "        n_samples, n_features = X_train_data.shape\n",
    "        self._classes = np.unique(y_train)\n",
    "        y_train = np.array(y_train)\n",
    "        lb = preprocessing.LabelBinarizer()\n",
    "        y_train = lb.fit_transform(y_train)\n",
    "        self.count_matrix = np.matmul(y_train.T,X_train_data)\n",
    "        self.class_count = y_train.sum(axis=0)\n",
    "        n_classes = len(self._classes)\n",
    "        self.log_probabilities = feature_log_probabilities(self.count_matrix)\n",
    "        print(self.log_probabilities)\n",
    "\n",
    "    def feature_log_probabilities(count_matrix):\n",
    "        alpha=1\n",
    "        # Adding alpha to the count\n",
    "        smoothed_version = count_matrix+alpha\n",
    "        print(smoothed_version)\n",
    "        # Calculating the number of words in a given class\n",
    "        den = smoothed_version.sum(axis = 1)\n",
    "        # Reshaping it to 2D column\n",
    "        den = den.reshape(-1,1)\n",
    "        # probability is num/den -- log probability is log(num)- log(den)\n",
    "        log_probabilities = np.log(smoothed_version)-np.log(den)\n",
    "        return log_probabilities\n",
    "    \n",
    "    def calculate_prior_probs(class_count):\n",
    "        # class count - [8,10]\n",
    "        # probabilities will be 8/18, 10/18\n",
    "        # And we apply log operation on to them.\n",
    "        num = class_count\n",
    "        den = class_count.sum()\n",
    "        return np.log(num)-np.log(den)\n",
    "    \n",
    "    def predict(self, X_test_data):\n",
    "        y_pred = [self._predict(x, self.log_probabilities, self.class_count) for x in X_test_data]\n",
    "#         print (y_pred)\n",
    "        return y_pred\n",
    "    \n",
    "    def _predict(self, query_point,log_probabilities,classes):\n",
    "        # Calculating posterior probabilities\n",
    "        output = np.matmul(log_probabilities,query_point.T)\n",
    "        # Finding the index using argmax and returing the specified class.\n",
    "        index = np.argmax(output)\n",
    "        #print(index)\n",
    "        return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "1356b6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manually implemented log-probabilities\n",
      "\n",
      "[[-12.82019285 -12.82019285 -12.82019285 ... -11.02843338 -12.82019285\n",
      "  -12.82019285]\n",
      " [-11.93169282 -11.93169282 -11.08439496 ... -10.95086356 -11.93169282\n",
      "  -11.08439496]\n",
      " [-10.99507942 -12.38137378 -12.38137378 ... -10.77193587 -12.38137378\n",
      "  -12.38137378]]\n",
      "Manual predict [1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "MNB = naiveBayes()\n",
    "#MNB = MultinomialNB()\n",
    "print('Manually implemented log-probabilities\\n')\n",
    "MNB.fit(X_train_data, y_train)\n",
    "print('Manual predict', MNB.predict(X_test_data[0:1]))\n",
    "predictions = MNB.predict(X_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "1153888e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn feature log-probabilities\n",
      " [[-12.82019285 -12.82019285 -12.82019285 ... -11.02843338 -12.82019285\n",
      "  -12.82019285]\n",
      " [-11.93169282 -11.93169282 -11.08439496 ... -10.95086356 -11.93169282\n",
      "  -11.08439496]\n",
      " [-10.99507942 -12.38137378 -12.38137378 ... -10.77193587 -12.38137378\n",
      "  -12.38137378]]\n",
      "Sklearn predict [1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "#MNB = naiveBayes()\n",
    "MNB1 = MultinomialNB()\n",
    "MNB1.fit(X_train_data, y_train)\n",
    "predictions1 = MNB1.predict(X_test_data)\n",
    "# predictions = MNB.predict(X_test_data[122:124])\n",
    "# print(predictions)\n",
    "print('Sklearn feature log-probabilities\\n',MNB1.feature_log_prob_)\n",
    "# Comparing predict function\n",
    "print('Sklearn predict',MNB1.predict(X_test_data[0:1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "60bcee5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy of manual model:  0.81375\n",
      "Test accuracy of manual model: 0.7986666666666666\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.80      0.81      4065\n",
      "           1       0.76      0.70      0.73      3940\n",
      "           2       0.81      0.89      0.85      3995\n",
      "\n",
      "    accuracy                           0.80     12000\n",
      "   macro avg       0.80      0.80      0.80     12000\n",
      "weighted avg       0.80      0.80      0.80     12000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "print('Train accuracy of manual model: ',accuracy_score(y_train, MNB.predict(X_train_data)))\n",
    "print('Test accuracy of manual model:', accuracy_score(y_test, predictions))\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "63ccfd4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.8139166666666666\n",
      "Test accuracy: 0.7990833333333334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.80      0.82      4065\n",
      "           1       0.76      0.70      0.73      3940\n",
      "           2       0.80      0.89      0.85      3995\n",
      "\n",
      "    accuracy                           0.80     12000\n",
      "   macro avg       0.80      0.80      0.80     12000\n",
      "weighted avg       0.80      0.80      0.80     12000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "print('Train accuracy: ',accuracy_score(y_train, MNB1.predict(X_train_data)))\n",
    "print('Test accuracy:', accuracy_score(y_test, predictions1))\n",
    "print(classification_report(y_test, predictions1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "bfaac787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11631     chose phone many friends family love takes gre...\n",
       "84449              good phone accessories intact works fine\n",
       "35195     phone great condition problems getting activat...\n",
       "220144    phone display good quite flimsy aware internat...\n",
       "47134     phone perfect condition got everything fine pu...\n",
       "                                ...                        \n",
       "50168                                         glitches alot\n",
       "11921     anti iphone like things android cant get ill l...\n",
       "39539     came mail cracked screen nonoperational able g...\n",
       "291025    like phoneit arrived adequate date waorks good...\n",
       "23351     amazing fast like use use phone love phone any...\n",
       "Name: Reviews, Length: 12000, dtype: object"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "2bc76c0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>phone great condition problems getting activat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>phone display good quite flimsy aware internat...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>phone perfect condition got everything fine pu...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>excellent</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Sentiment\n",
       "0  phone great condition problems getting activat...          1\n",
       "1  phone display good quite flimsy aware internat...          2\n",
       "2  phone perfect condition got everything fine pu...          2\n",
       "3                                          excellent          1"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_tuples = list(zip(X_test[2:6], predictions))\n",
    "pd.DataFrame(list_of_tuples, columns = ['Text', 'Sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "ba724690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc:  0.8270625\n",
      "Test acc:  0.81375\n"
     ]
    }
   ],
   "source": [
    "print('Train acc: ',accuracy_score(y_train, MNB.predict(X_train_data)))\n",
    "print('Test acc: ', accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "9a89869d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3422,  517,  126],\n",
       "       [ 561, 2980,  399],\n",
       "       [ 189,  443, 3363]])"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, predictions)\n",
    "\n",
    "# it shoes that out of 1012 data, 112 data that are acutally positive is predicted as negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6f2df2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11838d6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "97096eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = [\n",
    "#         'Will not repurchase. Do not recommend',\n",
    "#        'I hate this',\n",
    "#        'I love this',\n",
    "#        'This is not worth it',\n",
    "#        'This is not terrible',\n",
    "#        'This product so far has not disappointed',\n",
    "        'Excellent product. Easy to use, large screen makes watching movies and reading easier.',\n",
    "        'I am so happy today',\n",
    "       'Note 10 has great camera quality. I am loving it.',\n",
    "       'I dont know what is wrong with this phone. I have been trying to type but its not working.',\n",
    "       'Dell laptop battery dead',\n",
    "        'Good, but unhappy that screen size is small, less than I expected',\n",
    "       'The product has not disappointed',\n",
    "       'It is a expensive phone',\n",
    "       'I hate apple',\n",
    "        'less expensive than last year with so many more features and bigger screen!',\n",
    "        'I do not enjoy working under tight deadline',\n",
    "        'worst phone in the history',\n",
    "        'phone is not good but has nice screen'\n",
    "]\n",
    "# print(text1)\n",
    "text_df1 = pd.DataFrame(text1,columns=['text'])\n",
    "# print(text_df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "ad6ad82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df1['text'] =text_df1['text'].apply(lower_case)\n",
    "text_df1['text'] =text_df1['text'].apply(remove_multiple)\n",
    "text_df1['text'] =text_df1['text'].apply(remove_single_char)\n",
    "text_df1['text'] =text_df1['text'].apply(remove_special_characters)\n",
    "# text_df1['text'] =text_df1['text'].apply(remove_stopwords)\n",
    "text_df1['text'] =text_df1['text'].apply(remove_square_brackets)\n",
    "text_df1['text'] =text_df1['text'].apply(remove_urls)\n",
    "text_df1['text'] =text_df1['text'].apply(remove_username)\n",
    "text_df1['text'] =text_df1['text'].apply(replace_contractions)\n",
    "text_df1['text'] =text_df1['text'].apply(replace_negation)\n",
    "# text_df1['text'] =text_df1['text'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "bac688e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "text=tfidf_vector.transform(text_df1['text'])\n",
    "print(type(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "ea2bb5d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 does not have enough dimensions (has 0, gufunc core with signature (n?,k),(k,m?)->(n?,m?) requires 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/qy/ltftvghn3vb8lhkfmh8m0tx40000gn/T/ipykernel_2668/2698171567.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMNB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/qy/ltftvghn3vb8lhkfmh8m0tx40000gn/T/ipykernel_2668/4234541269.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X_test_data)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_probabilities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_count\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_test_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;31m#         print (y_pred)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/qy/ltftvghn3vb8lhkfmh8m0tx40000gn/T/ipykernel_2668/4234541269.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_probabilities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_count\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_test_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;31m#         print (y_pred)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/qy/ltftvghn3vb8lhkfmh8m0tx40000gn/T/ipykernel_2668/4234541269.py\u001b[0m in \u001b[0;36m_predict\u001b[0;34m(self, query_point, log_probabilities, classes)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_point\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlog_probabilities\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m# Calculating posterior probabilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_probabilities\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mquery_point\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;31m# Finding the index using argmax and returing the specified class.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 does not have enough dimensions (has 0, gufunc core with signature (n?,k),(k,m?)->(n?,m?) requires 1)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "e09446b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sentences = [\n",
    "    'I like this phone', \n",
    "    'The screen is small in this phone', \n",
    "    'I prefer big screen over small in a phone'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "7c8a65bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>big</th>\n",
       "      <th>in</th>\n",
       "      <th>is</th>\n",
       "      <th>like</th>\n",
       "      <th>over</th>\n",
       "      <th>phone</th>\n",
       "      <th>prefer</th>\n",
       "      <th>screen</th>\n",
       "      <th>small</th>\n",
       "      <th>the</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.720333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.425441</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.547832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.352215</td>\n",
       "      <td>0.463121</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.273526</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.352215</td>\n",
       "      <td>0.352215</td>\n",
       "      <td>0.463121</td>\n",
       "      <td>0.352215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.443503</td>\n",
       "      <td>0.337295</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.443503</td>\n",
       "      <td>0.261940</td>\n",
       "      <td>0.443503</td>\n",
       "      <td>0.337295</td>\n",
       "      <td>0.337295</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        big        in        is      like      over     phone    prefer  \\\n",
       "0  0.000000  0.000000  0.000000  0.720333  0.000000  0.425441  0.000000   \n",
       "1  0.000000  0.352215  0.463121  0.000000  0.000000  0.273526  0.000000   \n",
       "2  0.443503  0.337295  0.000000  0.000000  0.443503  0.261940  0.443503   \n",
       "\n",
       "     screen     small       the      this  \n",
       "0  0.000000  0.000000  0.000000  0.547832  \n",
       "1  0.352215  0.352215  0.463121  0.352215  \n",
       "2  0.337295  0.337295  0.000000  0.000000  "
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "vectorizer = tfidf.fit_transform(sample_sentences)\n",
    "pd.DataFrame(vectorizer.toarray(), columns=tfidf.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f1ad81",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_tuples = list(zip(text_df1['text'], prediction))\n",
    "pd.DataFrame(list_of_tuples, columns = ['Text', 'Sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7598d30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac41a02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
